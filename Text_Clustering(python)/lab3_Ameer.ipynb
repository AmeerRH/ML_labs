{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import regular expressins packge\n",
        "# import numbers package\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "JZ-BUBW7n6xa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read file\n",
        "def readFile(fileName):\n",
        "    file = open(fileName,'r',encoding=\"cp437\")\n",
        "    fileText = \"\"\n",
        "    for line in file:\n",
        "        fileText += line\n",
        "    return fileText"
      ],
      "metadata": {
        "id": "gWMpvhX6n6uu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "def preProcess(text):\n",
        "    # Remove non-letter chars\n",
        "    text = re.sub(\"[^a-zA-Z ]\",\" \", text)\n",
        "    # Change characters to lower\n",
        "    text = text.lower()\n",
        "\n",
        "    text = \" \".join(word for word in text.split() if len(word) > 2)\n",
        "    return text"
      ],
      "metadata": {
        "id": "r-RU0VXCn6sB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genReferenceWordList(texts,stopWords):\n",
        "# concatenate the texts\n",
        "    allText = \"\"\n",
        "    for line in texts:\n",
        "        allText += line\n",
        "    # Generate a word list\n",
        "    wordsList =  allText.split()\n",
        "    # Generate a word set\n",
        "    wordsSet =  set(wordsList)\n",
        "    # Remove the stop words from the word list\n",
        "    stopWordsList = stopWords.split()\n",
        "    stopWordsSet = set(stopWordsList)\n",
        "    refWordSet = wordsSet.difference(stopWordsSet)\n",
        "    return list(refWordSet)"
      ],
      "metadata": {
        "id": "Z4jTb5lMn6pc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vector distance\n",
        "# find the distance between arrays\n",
        "def vecDist(vec1,vec2):\n",
        "    vecDiff = vec1-vec2\n",
        "    # compute the distance (\"pitagoras\")\n",
        "    vecSqr =  np.square(vecDiff)\n",
        "    vecSum =  np.sum(vecSqr)\n",
        "    return np.sqrt(vecSum)"
      ],
      "metadata": {
        "id": "QPAcaH7qn6mw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vecDist_New(vec1, vec2):\n",
        "    vecDiff = vec1 - vec2\n",
        "    # compute the distance (sqrt of sum of absolute differences)\n",
        "    vecAbs = np.abs(vecDiff)\n",
        "    vecSum = np.sum(vecAbs)\n",
        "    return np.sqrt(vecSum)"
      ],
      "metadata": {
        "id": "keTkUVoEOilU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word frequency\n",
        "def getWordFrequency(texts,dictList):\n",
        "    dictSize = len(dictList)\n",
        "    nTexts = len(texts)\n",
        "    wordFreq = np.empty((nTexts,dictSize))\n",
        "    for i in range(nTexts):\n",
        "        print(\"text\" + str(i))\n",
        "        for j in range(dictSize):\n",
        "            wordFreq[i,j] = texts[i].count(dictList[j])\n",
        "    return wordFreq"
      ],
      "metadata": {
        "id": "7a56UQcNoQ-q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = []\n",
        "#read  and preprocess files\n",
        "books+= [readFile('DB.txt')]\n",
        "books+= [readFile('Eliot.txt')]\n",
        "books+= [readFile('HP.txt')]\n",
        "books+= [readFile('Tolkien.txt')]"
      ],
      "metadata": {
        "id": "-Dm1gm3OoQ8F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window = 150000"
      ],
      "metadata": {
        "id": "AGuA1zpvoQ5Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split every book into two parts\n",
        "bookParts = []\n",
        "for book in books:\n",
        "    nParts = len(book)//window\n",
        "    for i in range(nParts):\n",
        "        bookParts += [book[i*window:(i+1)*window]]"
      ],
      "metadata": {
        "id": "t9SSIjAloU7G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1on9eN_McXhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts in book parts\n",
        "texts = []\n",
        "for text in bookParts:\n",
        "    texts += [preProcess(text)]"
      ],
      "metadata": {
        "id": "P2Fqrop4oUyi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read stop words file - words that can be removed\n",
        "stopWords = readFile('stopwords_en.txt')"
      ],
      "metadata": {
        "id": "Mupvf2ujoYLz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate reference word list\n",
        "refList = genReferenceWordList(texts,stopWords)"
      ],
      "metadata": {
        "id": "9SrXG7gRoYDe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the frequency of the reference words in the files\n",
        "wordFreq = getWordFrequency(texts,refList)"
      ],
      "metadata": {
        "id": "jWIATzKhobfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245ab8fb-9c32-433b-e6c8-c8422acba59b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text0\n",
            "text1\n",
            "text2\n",
            "text3\n",
            "text4\n",
            "text5\n",
            "text6\n",
            "text7\n",
            "text8\n",
            "text9\n",
            "text10\n",
            "text11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-OBJg5O_n3gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75561fc6-5bb1-4479-fba8-65d31fd58793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dist matrix = \n",
            " [[   0.  611.  703. 1098. 1024. 1025. 1343. 1309. 1264. 1130. 1221. 1197.]\n",
            " [ 611.    0.  534. 1093. 1000. 1000. 1310. 1289. 1236. 1097. 1180. 1158.]\n",
            " [ 703.  534.    0. 1097. 1006. 1014. 1355. 1327. 1272. 1124. 1222. 1204.]\n",
            " [1098. 1093. 1097.    0.  520.  519. 1273. 1211. 1175.  949. 1054. 1034.]\n",
            " [1024. 1000. 1006.  520.    0.  477. 1210. 1167. 1110.  893. 1023.  995.]\n",
            " [1025. 1000. 1014.  519.  477.    0. 1207. 1155. 1103.  889.  997.  960.]\n",
            " [1343. 1310. 1355. 1273. 1210. 1207.    0.  592.  646. 1297. 1298. 1260.]\n",
            " [1309. 1289. 1327. 1211. 1167. 1155.  592.    0.  652. 1252. 1272. 1246.]\n",
            " [1264. 1236. 1272. 1175. 1110. 1103.  646.  652.    0. 1190. 1212. 1176.]\n",
            " [1130. 1097. 1124.  949.  893.  889. 1297. 1252. 1190.    0.  677.  667.]\n",
            " [1221. 1180. 1222. 1054. 1023.  997. 1298. 1272. 1212.  677.    0.  530.]\n",
            " [1197. 1158. 1204. 1034.  995.  960. 1260. 1246. 1176.  667.  530.    0.]]\n"
          ]
        }
      ],
      "source": [
        "# find the distance matrix between the text files\n",
        "# text distances\n",
        "rows,colomns = wordFreq.shape\n",
        "\n",
        "dist = np.zeros((rows,rows))\n",
        "for i in range(rows):\n",
        "    for j in range(rows):\n",
        "        # calculate the distance between the frequency vectors\n",
        "        dist[i,j] = round(vecDist(wordFreq[i],wordFreq[j]))\n",
        "\n",
        "print(\"dist matrix = \\n\",dist)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xxx_raw_text = readFile('xxx.txt')\n",
        "xxx_processed_text = preProcess(xxx_raw_text)\n",
        "print(\"Length of raw text: %d\" % len(xxx_raw_text))\n",
        "print(\"Length of processed text: %d\" % len(xxx_processed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5hwQQ1NU7t0",
        "outputId": "8500d32b-65bf-4d8e-d1ad-a65d4b8c013d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of raw text: 260892\n",
            "Length of processed text: 219150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xxx_wordFreq = getWordFrequency([xxx_processed_text], refList)\n",
        "print(f\"Shape of xxx_wordFreq: {xxx_wordFreq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he3YsQHjVFeH",
        "outputId": "12eb7756-18c6-42bd-f6a3-0fa075eb9daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distances_to_xxx = []\n",
        "num_book_parts = wordFreq.shape[0]\n",
        "\n",
        "for i in range(num_book_parts):\n",
        "    # vecDist function expects two vectors, xxx_wordFreq is (1, N) so we take its first row\n",
        "    distance = vecDist(wordFreq[i], xxx_wordFreq[0])\n",
        "    distances_to_xxx.append(round(distance))\n",
        "\n",
        "print(\"Distances from xxx.txt to each book part:\", distances_to_xxx)"
      ],
      "metadata": {
        "id": "ogFg37RNVpGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_distance = min(distances_to_xxx)\n",
        "min_distance_index = distances_to_xxx.index(min_distance)\n",
        "\n",
        "# Assuming each book was split into `my_nParts` (which is 3 from the context)\n",
        "book_titles = ['DB.txt', 'Eliot.txt', 'HP.txt', 'Tolkien.txt']\n",
        "book_part_size = 3 # from my_nParts which was len(book) // my_window (150000)\n",
        "\n",
        "# Determine which original book the closest part belongs to\n",
        "closest_book_index = min_distance_index // book_part_size\n",
        "closest_book_title = book_titles[closest_book_index]\n",
        "\n",
        "print(\"The minimum distance to xxx.txt is: %d\" % min_distance)\n",
        "print(\"This distance corresponds to book part index: %d\" % min_distance_index)\n",
        "print(\"The original book most similar to xxx.txt is: %s\" % closest_book_title)"
      ],
      "metadata": {
        "id": "S3e6lHfsV8bg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}